<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How I Learnt to Solve the Kaggle House Price Advanced Regression</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 850px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.7;
      color: #222;
    }
    h1, h2, h3 {
      color: #004080;
    }
    pre {
      background: #f4f4f4;
      padding: 1em;
      border-left: 4px solid #ccc;
      overflow-x: auto;
    }
    code {
      background: #eef;
      padding: 0.2em 0.5em;
      border-radius: 3px;
    }
    ul {
      padding-left: 1.2rem;
    }
    .book-note {
      background: #f9f9f9;
      border-left: 4px solid #4CAF50;
      padding: 1em;
      margin-top: 2em;
    }
    a {
      color: #0066cc;
    }
  </style>
</head>
<body>

  <h1>How I Learnt to Solve the Kaggle House Price Advanced Regression</h1>

  <p>
    This page tells the story of how I approached and solved the famous Kaggle competition on house price prediction.
    It includes the lessons I learned, a line-by-line walkthrough of the code, and helpful context for beginners looking to do the same.
  </p>

  <h2>‚Å∑ Step 1: Installing My Custom Cleaner</h2>
  <p>
    I built my own data cleaner and turned it into a Python package so I could reuse it easily:
  </p>
  <pre><code>pip install git+https://github.com/testgithubprecious/house_price_cleaner.git</code></pre>

  <p>
    The cleaner handles:
    <ul>
      <li>Ordinal encoding of quality-related features</li>
      <li>Missing value handling</li>
      <li>Feature selection and transformation</li>
    </ul>
  </p>

  <h2>Step 2: Full Code Walkthrough with Explanation</h2>

  <h3>Import Libraries</h3>
  <pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from house_price_cleaner.cleaner import HousePricePreprocessor</code></pre>
  <p>
    These libraries do everything from data loading, cleaning, splitting, training, and evaluating the model.
    My custom class <code>HousePricePreprocessor</code> is imported from the installed package.
  </p>

  <h3>Load the Dataset</h3>
  <pre><code>df = pd.read_csv("train.csv")
y = df["SalePrice"]
X = df.drop(columns=["SalePrice"])</code></pre>
  <p>
    We separate the target variable <code>SalePrice</code> into <code>y</code>, and keep the rest of the features in <code>X</code>.
  </p>

  <h3>Define Ordinal Mappings</h3>
  <pre><code>ordinal_mappings = {
  'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'PoolQC': {'NA': 0, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
}
ordinal_features = list(ordinal_mappings.keys())</code></pre>
  <p>
    These mappings convert quality features like <code>ExterQual</code> or <code>BsmtQual</code> into meaningful numbers where "Poor" is low and "Excellent" is high.
  </p>

  <h3>Preprocess the Data</h3>
  <pre><code>preprocessor = HousePricePreprocessor(ordinal_mappings, ordinal_features)
X_processed = preprocessor.fit_transform(X, y)</code></pre>
  <p>
    This applies the full cleaning logic on the input data and returns a model-ready dataset.
  </p>

  <h3>Split into Train/Validation Sets</h3>
  <pre><code>X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)</code></pre>
  <p>
    We split the data into 80% training and 20% validation. Using <code>random_state=42</code> keeps it reproducible.
  </p>

  <h3>Train the XGBoost Model</h3>
  <pre><code>model = XGBRegressor(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train)</code></pre>
  <p>
    This model is powerful and easy to tune. Here we use 100 trees and a learning rate of 0.1 for balance.
  </p>

  <h3>Make Predictions and Evaluate</h3>
  <pre><code>y_pred = model.predict(X_val)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
r2 = r2_score(y_val, y_pred)</code></pre>
  <p>
    RMSE (Root Mean Squared Error) tells us the average error in price predictions.<br>
    R¬≤ Score shows how much of the price variation the model explains.
  </p>

  <h3>Print the Results</h3>
  <pre><code>print("Validation RMSE:", rmse)
print("Validation R¬≤ Score:", r2)</code></pre>
  <p>
    Outputting results helps you track performance as you experiment and tweak parameters.
  </p>

  <h2>What I Learnt</h2>
  <ul>
    <li>Preprocessing is just as important as modeling</li>
    <li>Turning repetitive steps into packages makes projects faster</li>
    <li>XGBoost is great for beginners if you understand the basics</li>
  </ul>

  <div class="book-note">
    <p>
      If you're interested in learning <strong>how I built the <code>HousePricePreprocessor</code> class</strong> and why each step matters,
      I wrote a beginner-friendly guide that walks through it in detail ‚Äî from scratch to deployment.
    </p>
    <p><strong>üîó Check it out on Selar:</strong>
      <a href="https://selar.com/2y74rw3227?currency=USD" target="_blank">
        https://selar.com/2y74rw3227?currency=USD
      </a>
    </p>
  </div>

</body>
</html>

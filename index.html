<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How I Learnt to Solve the Kaggle House Price Advanced Regression</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-width: 850px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.7;
      color: #222;
    }
    h1, h2, h3 {
      color: #004080;
    }
    pre {
      position: relative;
      background: #f4f4f4;
      padding: 1em;
      border-left: 4px solid #ccc;
      overflow-x: auto;
    }
    code {
      background: #eef;
      padding: 0.2em 0.5em;
      border-radius: 3px;
      font-family: Consolas, Monaco, monospace;
    }
    ul {
      padding-left: 1.2rem;
    }
    .book-note {
      background: #f9f9f9;
      border-left: 4px solid #4CAF50;
      padding: 1em;
      margin-top: 2em;
    }
    a {
      color: #0066cc;
    }
    .copy-btn {
      position: absolute;
      top: 10px;
      right: 10px;
      background: #004080;
      color: #fff;
      border: none;
      padding: 4px 8px;
      font-size: 12px;
      cursor: pointer;
      border-radius: 3px;
    }
  </style>
</head>
<body>

  <h1>How I Learnt to Solve the Kaggle House Price Advanced Regression</h1>

  <p>
    This page tells the story of how I approached and solved the famous Kaggle competition on house price prediction.
    It includes the lessons I learned, a line-by-line walkthrough of the code, and helpful context for beginners looking to do the same.
  </p>

  <h2>Step 1: Installing My Custom Cleaner</h2>
  <p>
    I built my own data cleaner and turned it into a Python package so I could reuse it easily:
  </p>
  <pre><code>pip install git+https://github.com/testgithubprecious/house_price_cleaner.git</code><button class="copy-btn">Copy</button></pre>

  <p>
    The cleaner handles:
    <ul>
      <li>Separation of numerical and categorical features</li>
      <li>Comprehensive data cleaning and transformation</li>
      <li>Robust feature engineering for better model readiness</li>
      <li>Returns a clean, model-ready DataFrame compatible with any ML algorithm</li>
    </ul>
    You can use the class too by installing it from the link above.
  </p>

  <h2>Step 2: Full Code Walkthrough with Explanation</h2>

  <p>
    ðŸ”— <strong>Dataset used:</strong> 
    <a href="https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data" target="_blank">
      House Prices - Advanced Regression Techniques (Kaggle)
    </a>
  </p>

  <h3>Import Libraries</h3>
  <pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from house_price_cleaner.cleaner import HousePricePreprocessor</code><button class="copy-btn">Copy</button></pre>
  <p>
    These libraries do everything from data loading, cleaning, splitting, training, and evaluating the model.
    My custom class <code>HousePricePreprocessor</code> is imported from the installed package.
  </p>

  <h3>Load the Dataset</h3>
  <pre><code>df = pd.read_csv("train.csv")
y = df["SalePrice"]
X = df.drop(columns=["SalePrice"])</code><button class="copy-btn">Copy</button></pre>
  <p>
    We separate the target variable <code>SalePrice</code> into <code>y</code>, and keep the rest of the features in <code>X</code>.
  </p>

  <h3>Define Ordinal Mappings</h3>
  <pre><code>ordinal_mappings = {
  'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},
  'PoolQC': {'NA': 0, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
}
ordinal_features = list(ordinal_mappings.keys())</code><button class="copy-btn">Copy</button></pre>
  <p>
    The ordinal data was manually mapped according to what was written on the data description, so I specified it.
    These mappings convert quality features like <code>ExterQual</code> or <code>BsmtQual</code> into meaningful numbers where "Poor" is low and "Excellent" is high.
  </p>

  <h3>Preprocess the Data</h3>
  <pre><code>preprocessor = HousePricePreprocessor(ordinal_mappings, ordinal_features)
X_processed = preprocessor.fit_transform(X, y)</code><button class="copy-btn">Copy</button></pre>
  <p>
    This applies the full cleaning logic on the input data and returns a model-ready dataset.
  </p>

  <h3>Split into Train/Validation Sets</h3>
  <pre><code>X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)</code><button class="copy-btn">Copy</button></pre>
  <p>
    We split the data into 80% training and 20% validation. Using <code>random_state=42</code> keeps it reproducible.
  </p>

  <h3>Train the XGBoost Model</h3>
  <pre><code>model = XGBRegressor(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train)</code><button class="copy-btn">Copy</button></pre>
  <p>
    This model is powerful and easy to tune. Here we use 100 trees and a learning rate of 0.1 for balance.
  </p>

  <h3>Make Predictions and Evaluate</h3>
  <pre><code>y_pred = model.predict(X_val)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
r2 = r2_score(y_val, y_pred)</code><button class="copy-btn">Copy</button></pre>
  <p>
    RMSE (Root Mean Squared Error) tells us the average error in price predictions.<br>
    RÂ² Score shows how much of the price variation the model explains.
  </p>

  <h3>Print the Results</h3>
  <pre><code>print("Validation RMSE:", rmse)
print("Validation RÂ² Score:", r2)</code><button class="copy-btn">Copy</button></pre>
  <p>
    Outputting results helps you track performance as you experiment and tweak parameters.
  </p>

  <h2>Test on the Test CSV</h2>
  <pre><code>test_df = pd.read_csv("test.csv")
X_test_processed = preprocessor.transform(test_df)
test_predictions = model.predict(X_test_processed)

submission = pd.DataFrame({
    "Id": test_df["Id"],
    "SalePrice": test_predictions
})
submission.to_csv("submission.csv", index=False)</code><button class="copy-btn">Copy</button></pre>
  <p>
    After validating the model, you can use it on the actual test set provided by Kaggle.
    This generates predictions and saves them in the format required for submission.
  </p>

  <h2>What I Learnt</h2>
  <ul>
    <li>Preprocessing is just as important as modeling</li>
    <li>Turning repetitive steps into packages makes projects faster</li>
    <li>XGBoost is great for beginners if you understand the basics</li>
  </ul>

  <div class="book-note">
    <p>
      If you're interested in learning <strong>how I built the <code>HousePricePreprocessor</code> class</strong> and why each step matters,
      I wrote a beginner-friendly guide that walks through it in detail â€” from scratch to deployment.
    </p>
    <p><strong>ðŸ”— Check it out on Selar:</strong>
      <a href="https://selar.com/2y74rw3227?currency=USD" target="_blank">
        https://selar.com/2y74rw3227?currency=USD
      </a>
    </p>
  </div>

  <script>
    document.querySelectorAll('.copy-btn').forEach(btn => {
      btn.addEventListener('click', () => {
        const code = btn.previousElementSibling.innerText;
        navigator.clipboard.writeText(code).then(() => {
          btn.innerText = "Copied!";
          setTimeout(() => { btn.innerText = "Copy"; }, 2000);
        });
      });
    });
  </script>

</body>
</html>
